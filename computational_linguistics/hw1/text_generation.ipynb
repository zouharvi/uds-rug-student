{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bit8ccba5e9b5f64217ae438dbc1825b40c",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "The input data is first sentence segmented by NLTK's `PunktSentenceTokenizer` and then tokenized to words by `TreebankWordTokenizer`, source [here](http://www.nltk.org/api/nltk.tokenize.html). The output is then later detokenized by `TreebankWordDetokenizer` which makes the output look better. I hope this is not considered cheating, since the tokenizers and detokenizers are rule based and don't learn from the data.\n",
    "\n",
    "Furthermore, the text generation doesn't stop after exactly `N` tokens, but after at least `N` tokens have passed and the last generated token was an end of sentence token. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ngram import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "word_detokenize = TreebankWordDetokenizer().detokenize\n",
    "import random\n",
    "\n",
    "START_SYMBOL = \"<$>\"\n",
    "END_SYMBOL   = \"</$>\"\n",
    "\n",
    "def corpus(corpus, nlength, lower, rev):\n",
    "    with open(corpus, 'r') as f:\n",
    "        start_n = nlength-1 if not rev else 1\n",
    "        end_n = 1 if not rev else nlength-1\n",
    "        data = [word_tokenize(sent) for sent in sent_tokenize(f.read() if not lower else f.read().lower())]\n",
    "        random.shuffle(data)\n",
    "        out = [START_SYMBOL]*start_n + data[0]\n",
    "        for sent in data[1:]:\n",
    "            out += [END_SYMBOL]*end_n + [START_SYMBOL]*start_n + sent\n",
    "        out += [END_SYMBOL]*end_n\n",
    "    return out\n",
    "\n",
    "def create_story(corpusname, nlength=2, wordcount=100, rev=False, lower=False):\n",
    "    data = corpus(corpusname, nlength, lower, rev)\n",
    "    if rev:\n",
    "        data = data[::-1]\n",
    "    ngram = BasicNgram(\n",
    "        nlength, data,\n",
    "        start_symbol=START_SYMBOL if not rev else END_SYMBOL,\n",
    "        end_symbol=END_SYMBOL if not rev else START_SYMBOL,\n",
    "        pad_left=False,\n",
    "        pad_right=False\n",
    "    )\n",
    "    context = [ngram._start_symbol]*(nlength-1)\n",
    "    output = []\n",
    "    i = 0\n",
    "    while (i < wordcount or context[-1] != ngram._end_symbol):\n",
    "        word = ngram[tuple(context)].generate()\n",
    "        context = context[1:] + [word]\n",
    "        output.append(word)\n",
    "        i += 1\n",
    "    output = [x for x in output if x not in {START_SYMBOL, END_SYMBOL}]\n",
    "    if rev:\n",
    "        output = output[::-1]\n",
    "    output = word_detokenize(output).replace(' .', '.')\n",
    "    if lower:\n",
    "        output = list(output)\n",
    "        output[0] = output[0].upper()\n",
    "        for (i, char) in enumerate(output):\n",
    "            if char == '.' and i < len(output)-2:\n",
    "                output[i+2] = output[i+2].upper()\n",
    "        output = ''.join(output)\n",
    "    print(output)\n",
    "\n",
    "create_story('../data/Junglebook.txt', nlength=2)\n",
    "create_story('../data/King James Bible.txt', nlength=4)\n",
    "create_story('../data/poetryfoundation_clean.txt', nlength=4, wordcount=50)\n",
    "create_story('../data/ads.txt', nlength=3, wordcount=30)\n",
    "create_story('../data/ads.txt', nlength=4, wordcount=30, lower=True)\n",
    "create_story('../data/ads.txt', nlength=3, wordcount=50, rev=True)"
   ]
  },
  {
   "source": [
    "Junglebook, no post-editing:\n",
    "\n",
    "\n",
    "> Kala Nag's elephants swung him sat down on with a tiger will come here--I can't touch cattle because he has kept my own use them one such as soon as he was a hundred years, but unless you are dogs, you have told even I am proud, a line was not sleep to Kaa, transcribe and dropped him. Look out in deep sleep, panting among men will be freely available by Teddy's bridle-wise?\"Don't mistake me, as tiny fibers, widow will pay for the young calf (they called the bath-rooms of things made it very easy striking--raining on thy mother and he would shout together.\n",
    "\n",
    "\n",
    "King James Bible, no post-editing:\n",
    "\n",
    "\n",
    "> But Deborah Rebekah's nurse died, and all the cattle ringstraked. Some remove the landmarks; they violently take away flocks, and their sockets of brass four; their hooks of silver, sixteen sockets; two sockets under another board. yea, sweeter than honey to my mouth! And there came in two men, sons of Belial shall be all righteous: they shall bring unto the LORD. Then came Peter to him, saying, Let the priests, and they spoil it, so that they wearied themselves to find the door.\n",
    "\n",
    "\n",
    "\n",
    "Poem dataset, taken from [here](https://www.kaggle.com/tgdivy/poetry-foundation-poems). Newlines and fine detokenization post-edited:\n",
    "\n",
    "```\n",
    "because this was all right\n",
    "I'm taking some time out\n",
    "from being alive with daughters\n",
    "\n",
    "It's OK\n",
    "I'm impersonating a kiss of lilacs\n",
    "a murder of chance every mouse in the dust\n",
    "where I’d huddle at the foot\n",
    "of his name\n",
    "\n",
    "on thursday\n",
    "he’ll endure\n",
    "```\n",
    "\n",
    "The trigrams from the [commercials transcripts dataset](https://www.kaggle.com/kevinhartman0/advertisement-transcripts-from-various-industries) delivered suprisingly fluent results, no post-editing:\n",
    "\n",
    "> Respect yourself, call your Allstate agent can help you. Look to IBM - serving businesses of all wheel drive standard. What can be a little less time at the front.\n",
    "\n",
    "Lowercasing the data provided somewhat less coherent output, but it made the model less overfit on the data, allowing to use a quadgram model. Capitalized with a script.\n",
    "\n",
    "> Now at home. A shower that can dissolve a day. Stop, before the dodge you want goes to somebody else.\n",
    "\n",
    "Throughout this task we assumed that text should be generated left to right. But what if we reversed the direction? The result, without post-editing, is actually not that bad:\n",
    "\n",
    "> Designed to reduce pain, poor coloring. The world's fastest-reacting suspension system. Five sprays for instant relief that won't replace fast acting inhalers for sudden symptoms and should not be able to afford a necklace made of sand?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In the following snippets, the ngram models from different corpora are swichted after every word. Since the models don't implement any smoothing, there needs to be some solution for the case where the first model generating some context which the second model never saw. I experimented first with ending the sentence and starting a new one, which provided somewhat poetric looking outputs. The following solutions just takes the next ngram model in line (after some number of unsuccessful attempts)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Okay. That's where our people as I am a daughter. No one likes to be rushed. I love you for ourselves by living in one ear and let your children are not.\n"
     ]
    }
   ],
   "source": [
    "def create_story_roll(corpusnames, nlength=2, wordcount=100):\n",
    "    datas = [corpus(corpusname, nlength, False, False) for corpusname in corpusnames]\n",
    "    ngrams = [\n",
    "        BasicNgram(\n",
    "            nlength, data,\n",
    "            start_symbol=START_SYMBOL, end_symbol=END_SYMBOL,    \n",
    "            pad_left=False, pad_right=False\n",
    "        )\n",
    "        for data in datas\n",
    "    ]\n",
    "    ngramcontexts = [set(ngram.contexts()) for ngram in ngrams]\n",
    "    context = [ngrams[0]._start_symbol]*(nlength-1)\n",
    "    output = []\n",
    "    \n",
    "    def present_in_all(word):\n",
    "        return all([word in context for context in ngramcontexts])\n",
    "\n",
    "    succ_i = 0\n",
    "    while (succ_i < wordcount or context[-1] != ngrams[0]._end_symbol):\n",
    "        current_ngram = ngrams.pop(0)\n",
    "        ngrams.append(current_ngram)\n",
    "        word = START_SYMBOL + START_SYMBOL # one token, does not exist anywhere in the data\n",
    "        tolerance = 20\n",
    "        skip_ngram_loop = False\n",
    "        while not present_in_all(tuple(context[1:]+[word])):\n",
    "            tolerance -= 1\n",
    "            if tolerance == 0:\n",
    "                skip_ngram_loop = True\n",
    "                # context = [ngrams[0]._start_symbol]*(nlength-1)\n",
    "                break\n",
    "            word = current_ngram[tuple(context)].generate()\n",
    "        if skip_ngram_loop:\n",
    "            continue\n",
    "        context = context[1:] + [word]\n",
    "        output.append(word)\n",
    "        succ_i += 1\n",
    "    output = [x for x in output if x not in {START_SYMBOL, END_SYMBOL}]\n",
    "    print(word_detokenize(output).replace(' .', '.'))\n",
    "\n",
    "create_story_roll(\n",
    "    ['../data/ads.txt', '../data/King James Bible.txt'],\n",
    "    nlength=2, wordcount=30\n",
    ")\n",
    "# This may take a few seconds to complete\n",
    "create_story_roll(\n",
    "    ['../data/ads.txt', '../data/poetryfoundation_clean.txt'],\n",
    "    nlength=3, wordcount=30\n",
    ")"
   ]
  },
  {
   "source": [
    "Commercials and King James Bible switching bigram model with no post-editing:\n",
    "\n",
    "> For in the army. And because his childhood and the ship our God I am rich more children of the house. Let me with the word for the knowledge and love which go into one man or my daughter, too mighty, saying I said.\n",
    "\n",
    "Commercials and Poems switching trigram model with only new lines added:\n",
    "\n",
    "```\n",
    "The same way again.\n",
    "I will go to bed.\n",
    "\n",
    "Discover the magic of the earth\n",
    "for some people are still in it.\n",
    "```\n",
    "\n",
    "I find this quite touching, especially the last sentence. No large ngrams were present in the original data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}