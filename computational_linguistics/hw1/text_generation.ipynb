{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bit8ccba5e9b5f64217ae438dbc1825b40c",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "The input data is first sentence segmented by NLTK's `PunktSentenceTokenizer` and then tokenized to words by `TreebankWordTokenizer`, source [here](http://www.nltk.org/api/nltk.tokenize.html). The output is then later detokenized by `TreebankWordDetokenizer` which makes the output look better. I hope this is not considered cheating, since the tokenizers and detokenizers are rule based and don't learn from the data.\n",
    "\n",
    "Furthermore, the text generation doesn't stop after exactly `N` tokens, but after at least `N` tokens have passed and the last generated token was an end of sentence token. \n",
    "\n",
    "I would wish to claim some extra points for working with reverse n-grams, multiple external datasets and creating and  switching ngram model. I also experimented with lower-casing the input data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ngram import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "word_detokenize = TreebankWordDetokenizer().detokenize\n",
    "import random\n",
    "\n",
    "START_SYMBOL = \"<$>\"\n",
    "END_SYMBOL   = \"</$>\"\n",
    "\n",
    "def corpus(corpus, lower):\n",
    "    with open(corpus, 'r') as f:\n",
    "        data = [word_tokenize(sent) for sent in sent_tokenize(f.read() if not lower else f.read().lower())]\n",
    "        random.shuffle(data)\n",
    "        out = [START_SYMBOL] + data[0]\n",
    "        for sent in data[1:]:\n",
    "            out += [END_SYMBOL, START_SYMBOL] + sent\n",
    "        out.append(END_SYMBOL)\n",
    "    return out\n",
    "\n",
    "def create_story(corpusname, nlength=2, wordcount=100, rev=False, lower=False):\n",
    "    data = corpus(corpusname, lower)\n",
    "    if rev:\n",
    "        data = data[::-1]\n",
    "    ngram = BasicNgram(\n",
    "        nlength, data,\n",
    "        start_symbol=START_SYMBOL if not rev else END_SYMBOL,\n",
    "        end_symbol=END_SYMBOL if not rev else START_SYMBOL\n",
    "    )\n",
    "    context = [ngram._start_symbol]*(nlength-1)\n",
    "    output = []\n",
    "    i = 0\n",
    "    while (i < wordcount or context[-1] != ngram._end_symbol):\n",
    "        word = ngram[tuple(context)].generate()\n",
    "        context = context[1:] + [word]\n",
    "        output.append(word)\n",
    "        i += 1\n",
    "    output = [x for x in output if x not in {START_SYMBOL, END_SYMBOL}]\n",
    "    if rev:\n",
    "        output = output[::-1]\n",
    "    output = word_detokenize(output).replace(' .', '.')\n",
    "    if lower:\n",
    "        output = list(output)\n",
    "        output[0] = output[0].upper()\n",
    "        for (i, char) in enumerate(output):\n",
    "            if char == '.' and i < len(output)-2:\n",
    "                output[i+2] = output[i+2].upper()\n",
    "        output = ''.join(output)\n",
    "    print(output)\n",
    "\n",
    "create_story('../data/Junglebook.txt', nlength=2)\n",
    "create_story('../data/King James Bible.txt', nlength=4)\n",
    "create_story('../data/poetryfoundation_clean.txt', nlength=4, wordcount=50)\n",
    "create_story('../data/ads.txt', nlength=3, wordcount=30)\n",
    "create_story('../data/ads.txt', nlength=4, wordcount=30, lower=True)\n",
    "create_story('../data/ads.txt', nlength=3, wordcount=50, rev=True)"
   ]
  },
  {
   "source": [
    "Junglebook, no post-editing:\n",
    "\n",
    "\n",
    "> Kala Nag's elephants swung him sat down on with a tiger will come here--I can't touch cattle because he has kept my own use them one such as soon as he was a hundred years, but unless you are dogs, you have told even I am proud, a line was not sleep to Kaa, transcribe and dropped him. Look out in deep sleep, panting among men will be freely available by Teddy's bridle-wise?\"Don't mistake me, as tiny fibers, widow will pay for the young calf (they called the bath-rooms of things made it very easy striking--raining on thy mother and he would shout together.\n",
    "\n",
    "\n",
    "King James Bible, no post-editing:\n",
    "\n",
    "\n",
    "> And Jacob awaked out of his mouth goeth a sharp sword, that our wives and our children's: now then, whatsoever God doeth, it shall be cast the same in hope, that he shut up the kingdom to Israel? And the Lord shall cover him all the while that it is an heave offering: and every laver was four cubits: and the men of Keilah deliver me up into the hand of her enemies. The LORD is far from me. For if thou wert cut out of the king of Samaria.\n",
    "\n",
    "\n",
    "\n",
    "Poem dataset, taken from [here](https://www.kaggle.com/tgdivy/poetry-foundation-poems). Newlines and fine detokenization post-edited:\n",
    "\n",
    "```\n",
    "because this was all right\n",
    "I'm taking some time out\n",
    "from being alive with daughters\n",
    "\n",
    "It's OK\n",
    "I'm impersonating a kiss of lilacs\n",
    "a murder of chance every mouse in the dust\n",
    "where I’d huddle at the foot\n",
    "of his name\n",
    "\n",
    "on thursday\n",
    "he’ll endure\n",
    "```\n",
    "\n",
    "The trigrams from the [commercials transcripts dataset](https://www.kaggle.com/kevinhartman0/advertisement-transcripts-from-various-industries) delivered suprisingly fluent results, no post-editing:\n",
    "\n",
    "> Respect yourself, call your Allstate agent can help you. Look to IBM - serving businesses of all wheel drive standard. What can be a little less time at the front.\n",
    "\n",
    "Lowercasing the data provided somewhat less coherent output, but it made the model less overfit on the data, allowing to use a quadgram model. Capitalized with a script.\n",
    "\n",
    "> Display the flag proudly. Gmac not only has money available, but at interest rates that make sense. They call me up.\n",
    "\n",
    "Throughout this task we assumed that text should be generated left to right. But what if we reversed the direction? The result, without post-editing, is actually not that bad:\n",
    "\n",
    "> Unlike ordinary toothpaste, you discern not only help you make it the Best Luxury SUV in 2014. You'll easily see the signs of Alzheimer's Center on site for those at least 18 who have felt this beautiful. No cars."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_story_roll(corpusnames, nlength=2, wordcount=100):\n",
    "    datas = [corpus(corpusname) for corpusname in corpusnames]\n",
    "    ngrams = [BasicNgram(nlength, data, start_symbol=START_SYMBOL, end_symbol=END_SYMBOL) for data in datas]\n",
    "    ngramcontexts = [set(ngram.contexts()) for ngram in ngrams]\n",
    "    context = [ngrams[0]._start_symbol]*(nlength-1)\n",
    "    output = []\n",
    "    i = 0\n",
    "    def present_in_all(word):\n",
    "        for context in ngramcontexts:\n",
    "            if word not in context:\n",
    "                return False\n",
    "        return True\n",
    "    while (i < wordcount or context[-1] != ngrams[0]._end_symbol):\n",
    "        current_ngram = ngrams.pop(0)\n",
    "        ngrams.append(current_ngram)\n",
    "        word = START_SYMBOL + START_SYMBOL # does not exist anywhere in the data\n",
    "        tolerance = 100\n",
    "        while not present_in_all(tuple(context[1:]+[word])):\n",
    "            tolerance -= 1\n",
    "            if tolerance == 0:\n",
    "                context = [ngrams[0]._start_symbol]*(nlength-1)\n",
    "            word = current_ngram[tuple(context)].generate()\n",
    "        context = context[1:] + [word]\n",
    "        output.append(word)\n",
    "        i += 1\n",
    "    output = [x for x in output if x not in {START_SYMBOL, END_SYMBOL}]\n",
    "    print(word_detokenize(output).replace(' .', '.'))\n",
    "\n",
    "create_story_roll(\n",
    "    ['../data/ads.txt', '../data/King James Bible.txt'],\n",
    "    nlength=2, wordcount=30\n",
    ")\n",
    "create_story_roll(\n",
    "    ['../data/ads.txt', '../data/poetryfoundation_clean.txt'],\n",
    "    nlength=3, wordcount=30\n",
    ")"
   ]
  },
  {
   "source": [
    "Commercials and King James Bible switching bigram model with no post-editing:\n",
    "\n",
    "> Our hands, and easy, sealing the wheel over ten of us, our sister, and enjoy pleasure. I run after all that simple pass on this thing: What I do them.\n",
    "\n",
    "Commercials and Poems switching trigram model with no post-editing:\n",
    "\n",
    "> Leave a And best. The great ones who look up to the world to contemplate And it isn't matter which Leave a Leave a Leave your shoes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}