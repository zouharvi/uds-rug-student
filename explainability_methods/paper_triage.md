# attention
- two years _before_ Attention Is All You Need
- introduction to _global_ and _local_ attention
- 8/10, interesting, yet easy

# LIME
- motivation for explainability
- introduction to their explainer system, which can be used even for blackboxes
- 7/10, interesting, slightly difficult

# SHAP
- ensemble for explainer systems
- 6/10, less interesting, slightly difficult

# LRP theory
- introduction to relevance propagation
- 6/10, interesting, difficult

# LRP application
- word significance/contribution
- 6/10, interesting, long

# Sensitivity Analysis, Activation Maximization
- 3/10, graphics

# Sensitivity Analysis, Application
- mystery
- 3/10

# Deconvolution and Pertrubation
- 3/10, graphics

# Text Deconvolution
- visualization of detected linguistic information
- 6/10

# Meaningful Pertrubation
- 3/10, graphics

# Pertrubation for NLP
- 4/10, topics from other areas, long

# DeepLIFT
- Contributing features
- 4/10, topics from other areas, difficult

# Integrated gradients
- 2/10, difficult

# Activation Maximization with GAN
- 2/10, graphics, long

# Activation Maximization, NLP
- 6/10, short

# Generating Visual Explanations
- 5/10, long and includes graphics, but interesting

# Generating textual explanations for text classification
- 6/10, difficult, but interesting

# Influence functions
- 2/10, long, difficult

# Influence functions for NLP
- 6/10, difficult, interesting

# Probing
- 3/10, difficult
